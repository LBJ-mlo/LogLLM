#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import json
import numpy as np
from transformers import AutoTokenizer, AutoModel
from moe_log_detector import MoEConfig, EncoderWithMoE

class LogAnomalyDetector:
    def __init__(self, model_path="./final_moe_model", bert_path="./bert-base-uncased"):
        """
        åˆå§‹åŒ–æ—¥å¿—å¼‚å¸¸æ£€æµ‹å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„MoEæ¨¡å‹è·¯å¾„
            bert_path: BERTæ¨¡å‹è·¯å¾„
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½BERTæ¨¡å‹å’Œåˆ†è¯å™¨
        print("åŠ è½½BERTæ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(bert_path)
        self.bert_model = AutoModel.from_pretrained(bert_path)
        self.bert_model = self.bert_model.to(self.device)
        self.bert_model.eval()
        
        # åŠ è½½MoEæ¨¡å‹
        print("åŠ è½½MoEæ¨¡å‹...")
        self.moe_model = EncoderWithMoE.from_pretrained(model_path)
        self.moe_model = self.moe_model.to(self.device)
        self.moe_model.eval()
        
        # æ ‡ç­¾æ˜ å°„
        self.label_map = {0: "æ­£å¸¸", 1: "å¼‚å¸¸"}
        
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def preprocess_log_sequence(self, log_sequence, max_log_entry_length=128, max_sequence_length=32):
        """
        é¢„å¤„ç†æ—¥å¿—åºåˆ—
        
        Args:
            log_sequence: æ—¥å¿—åºåˆ—å­—ç¬¦ä¸²ï¼Œç”¨æ¢è¡Œç¬¦åˆ†éš”
            max_log_entry_length: å•æ¡æ—¥å¿—æœ€å¤§é•¿åº¦
            max_sequence_length: åºåˆ—æœ€å¤§é•¿åº¦
            
        Returns:
            é¢„å¤„ç†åçš„å¼ é‡
        """
        log_entries = log_sequence.strip().split('\n')
        embeddings_list = []
        
        for log_entry in log_entries:
            # åˆ†è¯
            encoding = self.tokenizer(
                log_entry,
                max_length=max_log_entry_length,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )
            
            encoding = {k: v.to(self.device) for k, v in encoding.items()}
            
            # è·å–BERTåµŒå…¥
            with torch.no_grad():
                outputs = self.bert_model(
                    input_ids=encoding["input_ids"],
                    attention_mask=encoding["attention_mask"]
                )
                last_hidden_state = outputs.last_hidden_state
                # ä½¿ç”¨CLS token
                embedding = last_hidden_state[:, 0, :].squeeze(0)
                embeddings_list.append(embedding.cpu())
        
        # å †å åµŒå…¥
        precomputed_embeddings = torch.stack(embeddings_list)
        
        # åˆ›å»ºåºåˆ—çº§åˆ«çš„æ³¨æ„åŠ›æ©ç 
        seq_length = precomputed_embeddings.shape[0]
        sequence_attention_mask = torch.ones(seq_length, dtype=torch.bool)
        
        # æˆªæ–­æˆ–å¡«å……åˆ° max_sequence_length
        if seq_length > max_sequence_length:
            precomputed_embeddings = precomputed_embeddings[:max_sequence_length, :]
            sequence_attention_mask = sequence_attention_mask[:max_sequence_length]
        else:
            padding_needed = max_sequence_length - seq_length
            padding_tensor = torch.zeros(
                padding_needed,
                precomputed_embeddings.shape[1],
                dtype=precomputed_embeddings.dtype
            )
            padding_mask = torch.zeros(padding_needed, dtype=torch.bool)
            precomputed_embeddings = torch.cat([precomputed_embeddings, padding_tensor], dim=0)
            sequence_attention_mask = torch.cat([sequence_attention_mask, padding_mask], dim=0)
        
        return precomputed_embeddings.unsqueeze(0), sequence_attention_mask.unsqueeze(0)
    
    def predict(self, log_sequence):
        """
        é¢„æµ‹æ—¥å¿—åºåˆ—æ˜¯å¦å¼‚å¸¸
        
        Args:
            log_sequence: æ—¥å¿—åºåˆ—å­—ç¬¦ä¸²
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        # é¢„å¤„ç†
        embeddings, attention_mask = self.preprocess_log_sequence(log_sequence)
        embeddings = embeddings.to(self.device)
        attention_mask = attention_mask.to(self.device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = self.moe_model(
                precomputed_embeddings=embeddings,
                #attention_mask=attention_mask
            )
            logits = outputs["logits"]
            # æ£€æŸ¥logitsæ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(logits).any() or torch.isinf(logits).any():
                print("è­¦å‘Š: æ£€æµ‹åˆ°æ— æ•ˆçš„logitså€¼ï¼Œä½¿ç”¨é»˜è®¤é¢„æµ‹")
                return {
                    "prediction": "æ­£å¸¸",
                    "confidence": 0.5,
                    "probabilities": {"æ­£å¸¸": 0.5, "å¼‚å¸¸": 0.5},
                    "logits": [0.0, 0.0],
                    "warning": "æ¨¡å‹è¾“å‡ºåŒ…å«NaN/Infå€¼"
                }
            
            probabilities = torch.softmax(logits, dim=1)
            prediction = torch.argmax(logits, dim=1).item()
            confidence = probabilities[0][prediction].item()
            
            # ç¡®ä¿æ¦‚ç‡å€¼æœ‰æ•ˆ
            if torch.isnan(probabilities).any() or torch.isinf(probabilities).any():
                print("è­¦å‘Š: æ£€æµ‹åˆ°æ— æ•ˆçš„æ¦‚ç‡å€¼")
                probabilities = torch.tensor([[0.5, 0.5]], device=self.device)
                confidence = 0.5
        
        return {
            "prediction": self.label_map[prediction],
            "confidence": confidence,
            "probabilities": {
                "æ­£å¸¸": probabilities[0][0].item(),
                "å¼‚å¸¸": probabilities[0][1].item()
            },
            "logits": logits[0].cpu().numpy().tolist()
        }

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ¨ç†è¿‡ç¨‹"""
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = LogAnomalyDetector()
    
    # ç¤ºä¾‹æ—¥å¿—åºåˆ—
    example_logs = [
        {
            "name": "æ­£å¸¸æ—¥å¿—åºåˆ—",
            "logs": """2024-01-01 10:00:01 INFO [System] Application started successfully
2024-01-01 10:00:02 INFO [Database] Connection established
2024-01-01 10:00:03 INFO [API] Server listening on port 8080
2024-01-01 10:00:04 INFO [Cache] Redis cache initialized
2024-01-01 10:00:05 INFO [Auth] Authentication service ready"""
        },
        {
            "name": "å¼‚å¸¸æ—¥å¿—åºåˆ— - æ•°æ®åº“è¿æ¥å¤±è´¥",
            "logs": """2024-01-01 10:00:01 INFO [System] Application started successfully
2024-01-01 10:00:02 ERROR [Database] Connection failed: timeout
2024-01-01 10:00:03 ERROR [Database] Retry connection failed
2024-01-01 10:00:04 ERROR [Database] Database service unavailable
2024-01-01 10:00:05 WARN [System] Application running in degraded mode"""
        },
        {
            "name": "å¼‚å¸¸æ—¥å¿—åºåˆ— - APIé”™è¯¯",
            "logs": """2024-01-01 10:00:01 INFO [System] Application started successfully
2024-01-01 10:00:02 INFO [API] Server listening on port 8080
2024-01-01 10:00:03 ERROR [API] 500 Internal Server Error
2024-01-01 10:00:04 ERROR [API] Request timeout
2024-01-01 10:00:05 ERROR [API] Service unavailable"""
        },
        {
            "name": "å¼‚å¸¸æ—¥å¿—åºåˆ— - æƒé™é”™è¯¯",
            "logs": """2024-01-01 10:00:01 INFO [System] Application started successfully
2024-01-01 10:00:02 INFO [Auth] User login attempt
2024-01-01 10:00:03 ERROR [Auth] Permission denied: insufficient privileges
2024-01-01 10:00:04 ERROR [Auth] Access denied to resource
2024-01-01 10:00:05 WARN [System] Security alert: unauthorized access attempt"""
        }
    ]
    
    print("=" * 80)
    print("æ—¥å¿—å¼‚å¸¸æ£€æµ‹æ¨ç†æ¼”ç¤º")
    print("=" * 80)
    
    # å¯¹æ¯ä¸ªç¤ºä¾‹è¿›è¡Œæ¨ç†
    for i, example in enumerate(example_logs, 1):
        print(f"\nç¤ºä¾‹ {i}: {example['name']}")
        print("-" * 60)
        print("è¾“å…¥æ—¥å¿—åºåˆ—:")
        print(example['logs'])
        print("\næ¨ç†ç»“æœ:")
        
        try:
            result = detector.predict(example['logs'])
            print(f"é¢„æµ‹ç»“æœ: {result['prediction']}")
            print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f}")
            print(f"æ¦‚ç‡åˆ†å¸ƒ:")
            print(f"  æ­£å¸¸: {result['probabilities']['æ­£å¸¸']:.4f}")
            print(f"  å¼‚å¸¸: {result['probabilities']['å¼‚å¸¸']:.4f}")
            
            # æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
            if 'warning' in result:
                print(f"âš ï¸  {result['warning']}")
            
            # æ·»åŠ é¢œè‰²æ ‡è¯†
            if result['prediction'] == "å¼‚å¸¸":
                print("ğŸ”´ æ£€æµ‹åˆ°å¼‚å¸¸ï¼")
            else:
                print("ğŸŸ¢ æ—¥å¿—æ­£å¸¸")
                
        except Exception as e:
            print(f"æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        
        print("-" * 60)
    
    print("\næ¨ç†æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main() 